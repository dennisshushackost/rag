{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipepline from Scratch \n",
    "\n",
    "RAG (Retrieval Augmented Generation) has to goal to take information and pass it to a Large Language Model (LLM) so it can generate outputs based on that information. \n",
    "\n",
    "* **Retrieval**: Find relevant information given a user query. I.e. What are the macronutrients and what do they do -> Retrives any passages of text related to the macronutrients from a nutritien textbook. \n",
    "* **Augmented**: We want to take the relevant information from our data and then augment our imput (prompt) to an LLM with that relevant information. \n",
    "* **Generation**: Take the first two stepes and pass them to an LLM for a good output. \n",
    "\n",
    "Why RAG? \n",
    "The main goal of RAG is to improve the generation output of LLMs.\n",
    "1. Prevent Hallucinations - LLMs are good at generating good looking text, however it may not be factual.\n",
    "RAG can help LLMs create text based on text that is factual. \n",
    "2. Many LLMs are trained on internet data, as such they have a good understanding of language. RAG allows us to use custom data. We can use customer support Q&A for chatting. We can retrieve relevant snippets of text for example. We can retrieve the snippets and then use an LLM to craft an answer from these snippets. \n",
    "3. Why run it locally. We do not have to wait for any transfers. Cost is another big factor. If we own our own hardware, we can save on large amounts of costs. Furthermore, there is no vendor locking, when we run our own software, hardware. If OpenAI or another large internet company shuts down, we can still run the buisness. Privacy - Id you have documentation, maybe you do not want to send it to an API. You want to setup an LLM and run it on your own hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we going to build?\n",
    "https://whimsical.com/simple-local-rag-workflow-39kToR3yNf7E8kY4sS2tjV\n",
    "\n",
    "1. Open a pdf document.\n",
    "2. Format the text of the PDF textbook ready for an embedding model.\n",
    "3. Embed all the chunk of text in the textbook and turn them into numerical representations (embedding) which can store for later. \n",
    "4. Build retrieval system that uses vector search to find the relevant chunks of text based on query. \n",
    "5. Create a prompt that incorperates the retrieved prieces of text. \n",
    "6. Generate the answer to a query based on the passages based on the passages of the textbook with an LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document pre-processing and embedding creation \n",
    "\n",
    "Ingridients: PDF document of choice (could be any kind of document.) and an embedding model of choice. \n",
    "1. Import PDF document\n",
    "2. Process text for embedding (splitting into chunks of sentences)\n",
    "3. Embedd textchunks with embedding model.\n",
    "4. Save embedding to file for later (embeddings will store on file for many years until you loose them on hd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The file already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennis/Documents/GitHub/rag/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Programatically get the pdf document \n",
    "import os \n",
    "import requests \n",
    "\n",
    "# Get PDF document:\n",
    "pdf_path = \"./data/human-nutrition-text.pdf\"\n",
    "\n",
    "# Download the PDF:\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(\"[INFO] File does not exist, downloading....\")\n",
    "\n",
    "    # Enter the URL of the PDF: \n",
    "    url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "    # Local Filename to save the file:\n",
    "    filename = pdf_path\n",
    "\n",
    "    # Send a GET request:\n",
    "    response = requests.get(url=url)\n",
    "\n",
    "    # Check if the request was successfull:\n",
    "    if response.status_code == 200:\n",
    "        # Open file and save it (wb = write binary)\n",
    "        with open(filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"[INFO] The file has been downloaded and saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"[INFO] Failed to download the file. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(f\"[INFO] The file already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a PDF as such we can open it. We can use PyMUPDF which seems to be the best for PDF reading with the best Text formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.9/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.9/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.9/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from nltk) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1c2ae1924840c59d1a79428a610b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pdfplumber # MIT Licence \n",
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download NLTK data if not already present\n",
    "def download_nltk_data():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        print(\"Downloading NLTK punkt data...\")\n",
    "        nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Call the function to download NLTK data\n",
    "download_nltk_data()\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs basic formatting on text.\"\"\"\n",
    "    # Replace newlines and tabs with spaces\n",
    "    cleanted_text = text.replace('\\n', ' ').strip()\n",
    "    return cleanted_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics using NLTK.\n",
    "    \"\"\"\n",
    "    reader = pdfplumber.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(reader.pages)):\n",
    "        text = page.extract_text()\n",
    "        text = text_formatter(text)\n",
    "        \n",
    "        # Use NLTK for tokenization (Tokenize words and sentences)\n",
    "        words = word_tokenize(text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        pages_and_texts.append({\n",
    "            \"page_number\": page_number + 1,\n",
    "            \"page_char_count\": len(text),\n",
    "            \"page_word_count\": len(words),\n",
    "            \"page_sentence_count\": len(sentences),\n",
    "            \"page_token_count\": len(text) // 4,  # Approximation of Tokens 1 token = 4 char in eng.\n",
    "            \"text\": text\n",
    "        })\n",
    "    return pages_and_texts\n",
    "\n",
    "# Usage\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 493,\n",
       "  'page_char_count': 51,\n",
       "  'page_word_count': 12,\n",
       "  'page_sentence_count': 3,\n",
       "  'page_token_count': 12,\n",
       "  'text': 'PART VIII CHAPTER 8. ENERGY Chapter 8. Energy | 451'},\n",
       " {'page_number': 816,\n",
       "  'page_char_count': 1067,\n",
       "  'page_word_count': 169,\n",
       "  'page_sentence_count': 7,\n",
       "  'page_token_count': 266,\n",
       "  'text': 'Instead of… Replace with… Sweetened fruit Plain fat-free yogurt with fresh fruit yogurt Whole milk Low-fat or fat-free milk Cheese Low-fat or reduced-fat cheese Bacon or sausage Canadian bacon or lean ham Sweetened Minimally sweetened cereals with fresh fruit cereals Apple or berry Fresh apple or berries pie Deep-fried Oven-baked French fries or sweet potato baked fries French fries Fried vegetables Steamed or roasted vegetables Sugary sweetened soft Seltzer mixed with 100 percent fruit juice drinks Recipes that call Experiment with reducing amount of sugar and for sugar adding spices (cinnamon, nutmeg, etc…) Source: Food Groups. US Department of Agriculture. http://www.choosemyplate.gov/food-groups/. Updated April 19, 2017. Accessed November 22, 2017. Learning Activities Technology Note: The second edition of the Human Nutrition Open Educational Resource (OER) textbook features interactive learning activities. These activities are available in the web-based textbook and not available in the 774 | Understanding the Bigger Picture of Dietary Guidelines'},\n",
       " {'page_number': 373,\n",
       "  'page_char_count': 1731,\n",
       "  'page_word_count': 322,\n",
       "  'page_sentence_count': 20,\n",
       "  'page_token_count': 432,\n",
       "  'text': 'Tools for Change UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM Being conscious of the need to reduce cholesterol means limiting the consumption of saturated fats and trans fats. Remember that saturated fats found in some meat, whole-fat dairy products, and tropical oils elevate your total cholesterol. Trans fats, such as the ones often found in margarines, processed cookies, pastries, crackers, fried foods, and snack foods also elevate your cholesterol levels. Read and select from the following suggestions as you plan ahead: 1. Soluble fiber reduces cholesterol absorption in the bloodstream. Try eating more oatmeal, oat bran, kidney beans, apples, pears, citrus fruits, barley, and prunes. 2. Fatty fish are heart-healthy due to high levels of omega-3 fatty acids that reduce inflammation and lower cholesterol levels. Consume mackerel, lake trout, herring, sardines, tuna, salmon, and halibut. Grilling or baking is the best to avoid unhealthy trans fats that could be added from frying oil. 3. Walnuts, almonds, peanuts, hazelnuts, pecans, some pine nuts, and pistachios all contain high levels of unsaturated fatty acids that aid in lowering LDL. Make sure the nuts are raw and unsalted. Avoid sugary or salty nuts. One ounce each day is a good amount. 4. Olive oil contains a strong mix of antioxidants and monounsaturated fat, and may lower LDL while leaving HDL intact. Two tablespoons per day in place of less healthy saturated fats may contribute to these heart-healthy effects without adding extra calories. Extra virgin olive oil promises a greater effect, as the oil is minimally processed and contains more heart-healthy antioxidants. Tools for Change | 331'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>308</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>210</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>766</td>\n",
       "      <td>130</td>\n",
       "      <td>3</td>\n",
       "      <td>191</td>\n",
       "      <td>Contents Preface xxv University of Hawai‘i at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  ...                                               text\n",
       "0            1  ...                      Human Nutrition: 2020 Edition\n",
       "1            2  ...                                                   \n",
       "2            3  ...  Human Nutrition: 2020 Edition UNIVERSITY OF HA...\n",
       "3            4  ...  Human Nutrition: 2020 Edition by University of...\n",
       "4            5  ...  Contents Preface xxv University of Hawai‘i at ...\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>604.50</td>\n",
       "      <td>1121.18</td>\n",
       "      <td>201.30</td>\n",
       "      <td>10.55</td>\n",
       "      <td>279.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>552.32</td>\n",
       "      <td>100.37</td>\n",
       "      <td>6.58</td>\n",
       "      <td>138.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>302.75</td>\n",
       "      <td>741.50</td>\n",
       "      <td>130.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>185.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>604.50</td>\n",
       "      <td>1191.50</td>\n",
       "      <td>214.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>297.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>906.25</td>\n",
       "      <td>1572.50</td>\n",
       "      <td>282.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>393.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>2271.00</td>\n",
       "      <td>441.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>567.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  ...  page_sentence_count  page_token_count\n",
       "count      1208.00          1208.00  ...              1208.00           1208.00\n",
       "mean        604.50          1121.18  ...                10.55            279.92\n",
       "std         348.86           552.32  ...                 6.58            138.08\n",
       "min           1.00             0.00  ...                 0.00              0.00\n",
       "25%         302.75           741.50  ...                 5.00            185.00\n",
       "50%         604.50          1191.50  ...                10.00            297.50\n",
       "75%         906.25          1572.50  ...                15.00            393.00\n",
       "max        1208.00          2271.00  ...                30.00            567.00\n",
       "\n",
       "[8 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token count is very important, because:\n",
    "1. Embedding models do not deal with infinite tokens.\n",
    "2. LLMs do not deal with infinite tokens. \n",
    "\n",
    "For example an embedding model may have been trained to embedd sequences of 384  tokens. For this we will use 'all-mpnet-base-v2' to start off. \n",
    "\n",
    "As for LLMs, they cannot accept infinite tokens in their context window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Text Processing (Splitting pages into sentences)\n",
    "\n",
    "We can split our sentences into groups of ten sentences for example. We can use this using an NLP library (spaCy or NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This is another sentence., I lile elephants.]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English \n",
    "# We create a pipeline here: \n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline: (Turns text into sentences)\n",
    "# spacy.to/api/sentencizer\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create a documents instance:\n",
    "doc = nlp(\"This is a sentence. This is another sentence. I lile elephants.\")\n",
    "assert(len(list(doc.sents))==3)\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770d6fd6910341ac88bc710b982ff07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts): # is a dict\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "    # Make sure all the sentences are strings (defult type is spacy datatypes)\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "    # Count the sentences \n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 417,\n",
       "  'page_char_count': 1070,\n",
       "  'page_word_count': 188,\n",
       "  'page_sentence_count': 10,\n",
       "  'page_token_count': 267,\n",
       "  'text': 'Image by Annie Spratt on unspash.com / CC0 Protein Denaturation: Unraveling the Fold When a cake is baked, the proteins are denatured. Denaturation refers to the physical changes that take place in a protein exposed to abnormal conditions in the environment. Heat, acid, high salt concentrations, alcohol, and mechanical agitation can cause proteins to denature. When a protein denatures, its complicated folded structure unravels, and it becomes just a long strand of amino acids again. Weak chemical forces that hold tertiary and secondary protein structures together are broken when a protein is exposed to unnatural conditions. Because proteins’ function is dependent on their shape, denatured proteins are no longer functional. During cooking the applied heat causes proteins to vibrate. This destroys the weak bonds holding proteins in their complex shape (though this does not happen to the stronger peptide bonds). The unraveled protein strands then stick together, forming an aggregate (or network). The Role of Proteins in Foods: Cooking and Denaturation | 375',\n",
       "  'sentences': ['Image by Annie Spratt on unspash.com / CC0 Protein Denaturation: Unraveling the Fold When a cake is baked, the proteins are denatured.',\n",
       "   'Denaturation refers to the physical changes that take place in a protein exposed to abnormal conditions in the environment.',\n",
       "   'Heat, acid, high salt concentrations, alcohol, and mechanical agitation can cause proteins to denature.',\n",
       "   'When a protein denatures, its complicated folded structure unravels, and it becomes just a long strand of amino acids again.',\n",
       "   'Weak chemical forces that hold tertiary and secondary protein structures together are broken when a protein is exposed to unnatural conditions.',\n",
       "   'Because proteins’ function is dependent on their shape, denatured proteins are no longer functional.',\n",
       "   'During cooking the applied heat causes proteins to vibrate.',\n",
       "   'This destroys the weak bonds holding proteins in their complex shape (though this does not happen to the stronger peptide bonds).',\n",
       "   'The unraveled protein strands then stick together, forming an aggregate (or network).',\n",
       "   'The Role of Proteins in Foods: Cooking and Denaturation | 375'],\n",
       "  'page_sentence_count_spacy': 10}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>604.50</td>\n",
       "      <td>1121.18</td>\n",
       "      <td>201.30</td>\n",
       "      <td>10.55</td>\n",
       "      <td>279.92</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>552.32</td>\n",
       "      <td>100.37</td>\n",
       "      <td>6.58</td>\n",
       "      <td>138.08</td>\n",
       "      <td>6.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>302.75</td>\n",
       "      <td>741.50</td>\n",
       "      <td>130.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>185.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>604.50</td>\n",
       "      <td>1191.50</td>\n",
       "      <td>214.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>297.50</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>906.25</td>\n",
       "      <td>1572.50</td>\n",
       "      <td>282.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>393.00</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>2271.00</td>\n",
       "      <td>441.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>567.00</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  ...  page_sentence_count_spacy\n",
       "count      1208.00  ...                    1208.00\n",
       "mean        604.50  ...                      10.58\n",
       "std         348.86  ...                       6.60\n",
       "min           1.00  ...                       0.00\n",
       "25%         302.75  ...                       5.00\n",
       "50%         604.50  ...                      10.00\n",
       "75%         906.25  ...                      15.00\n",
       "max        1208.00  ...                      30.00\n",
       "\n",
       "[8 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking our sentences togheter:\n",
    "\n",
    "The concept of splitting larger pieces of text into smalles ones is ofter referred to as text splitting or chunking. There is no 100% correct way of doing this. We may also want to have a certain overlap inside our chunks. There are libraries, which help us do this. \n",
    "\n",
    "1. Helps us filter text (smalles groups of text can easier to inspect than larger ones.)\n",
    "2. So our text chunks can fit into the embedding model. \n",
    "3. So our context passed to an LLM can be more specific and focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [20, 21, 22, 23, 24]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define split size to turn groups of sentences into chunks \n",
    "num_sentence_chunk_size = 10 \n",
    "\n",
    "# SSplit list of text recursively into chunk size e-g-> 20 -> (10,10) (25) -> 10. 10. 5\n",
    "def split_list(input_list: list[str], slice_size: int) -> list[list[str]]:\n",
    "    return [input_list[i:i+slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "test_list = list(range(25))\n",
    "split_list(test_list, num_sentence_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56f8ee71841490eb5b3c17ad24a2954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through pages and text & split sentences into chunks: \n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 416,\n",
       "  'page_char_count': 1152,\n",
       "  'page_word_count': 205,\n",
       "  'page_sentence_count': 12,\n",
       "  'page_token_count': 288,\n",
       "  'text': 'The Role of Proteins in Foods: Cooking and Denaturation UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM In addition to having many vital functions within the body, proteins perform different roles in our foods by adding certain functional qualities to them. Protein provides food with structure and texture and enables water retention. For example, proteins foam when agitated. (Picture whisking egg whites to make angel food cake. The foam bubbles are what give the angel food cake its airy texture.) Yogurt is another good example of proteins providing texture. Milk proteins called caseins coagulate, increasing yogurt’s thickness. Cooked proteins add some color and flavor to foods as the amino group binds with carbohydrates and produces a brown pigment and aroma. Eggs are between 10 and 15 percent protein by weight. Most cake recipes use eggs because the egg proteins help bind all the other ingredients together into a uniform cake batter. The proteins aggregate into a network during mixing and baking that gives cake structure. 374 | The Role of Proteins in Foods: Cooking and Denaturation',\n",
       "  'sentences': ['The Role of Proteins in Foods: Cooking and Denaturation UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM In addition to having many vital functions within the body, proteins perform different roles in our foods by adding certain functional qualities to them.',\n",
       "   'Protein provides food with structure and texture and enables water retention.',\n",
       "   'For example, proteins foam when agitated. (',\n",
       "   'Picture whisking egg whites to make angel food cake.',\n",
       "   'The foam bubbles are what give the angel food cake its airy texture.)',\n",
       "   'Yogurt is another good example of proteins providing texture.',\n",
       "   'Milk proteins called caseins coagulate, increasing yogurt’s thickness.',\n",
       "   'Cooked proteins add some color and flavor to foods as the amino group binds with carbohydrates and produces a brown pigment and aroma.',\n",
       "   'Eggs are between 10 and 15 percent protein by weight.',\n",
       "   'Most cake recipes use eggs because the egg proteins help bind all the other ingredients together into a uniform cake batter.',\n",
       "   'The proteins aggregate into a network during mixing and baking that gives cake structure.',\n",
       "   '374 | The Role of Proteins in Foods: Cooking and Denaturation'],\n",
       "  'page_sentence_count_spacy': 12,\n",
       "  'sentence_chunks': [['The Role of Proteins in Foods: Cooking and Denaturation UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM In addition to having many vital functions within the body, proteins perform different roles in our foods by adding certain functional qualities to them.',\n",
       "    'Protein provides food with structure and texture and enables water retention.',\n",
       "    'For example, proteins foam when agitated. (',\n",
       "    'Picture whisking egg whites to make angel food cake.',\n",
       "    'The foam bubbles are what give the angel food cake its airy texture.)',\n",
       "    'Yogurt is another good example of proteins providing texture.',\n",
       "    'Milk proteins called caseins coagulate, increasing yogurt’s thickness.',\n",
       "    'Cooked proteins add some color and flavor to foods as the amino group binds with carbohydrates and produces a brown pigment and aroma.',\n",
       "    'Eggs are between 10 and 15 percent protein by weight.',\n",
       "    'Most cake recipes use eggs because the egg proteins help bind all the other ingredients together into a uniform cake batter.'],\n",
       "   ['The proteins aggregate into a network during mixing and baking that gives cake structure.',\n",
       "    '374 | The Role of Proteins in Foods: Cooking and Denaturation']],\n",
       "  'num_chunks': 2}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>604.50</td>\n",
       "      <td>1121.18</td>\n",
       "      <td>201.30</td>\n",
       "      <td>10.55</td>\n",
       "      <td>279.92</td>\n",
       "      <td>10.58</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>552.32</td>\n",
       "      <td>100.37</td>\n",
       "      <td>6.58</td>\n",
       "      <td>138.08</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>302.75</td>\n",
       "      <td>741.50</td>\n",
       "      <td>130.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>185.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>604.50</td>\n",
       "      <td>1191.50</td>\n",
       "      <td>214.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>297.50</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>906.25</td>\n",
       "      <td>1572.50</td>\n",
       "      <td>282.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>393.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>2271.00</td>\n",
       "      <td>441.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>567.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  ...  page_sentence_count_spacy  num_chunks\n",
       "count      1208.00          1208.00  ...                    1208.00     1208.00\n",
       "mean        604.50          1121.18  ...                      10.58        1.56\n",
       "std         348.86           552.32  ...                       6.60        0.68\n",
       "min           1.00             0.00  ...                       0.00        0.00\n",
       "25%         302.75           741.50  ...                       5.00        1.00\n",
       "50%         604.50          1191.50  ...                      10.00        1.00\n",
       "75%         906.25          1572.50  ...                      15.00        2.00\n",
       "max        1208.00          2271.00  ...                      30.00        3.00\n",
       "\n",
       "[8 rows x 7 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting each chunk into its own item:\n",
    "\n",
    "We'd like to embedd each chunk of sentences into its own numerical representation. This will give us a good level of granularity. Meaning, we can dive specifically into the text sample that was used in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb58ca03a5b443d90d79fef2734293f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1880"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "\n",
    "        # Join the sentences togheter into a paragraph structure => 1 paragrapg\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" => \". A\" \n",
    "\n",
    "\n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get some stats:\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4\n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 970,\n",
       "  'sentence_chunk': 'Rivlin, RS. (2007). Keeping the Young-Elderly Healthy: Is It Too Late to Improve Our Health through Nutrition?. American Journal of Clinical Nutrition, 86, 1572S–6S. 928 | Older Adulthood: The Golden Years',\n",
       "  'chunk_char_count': 205,\n",
       "  'chunk_word_count': 31,\n",
       "  'chunk_token_count': 51.25}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1880.00</td>\n",
       "      <td>1880.00</td>\n",
       "      <td>1880.00</td>\n",
       "      <td>1880.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>631.35</td>\n",
       "      <td>719.29</td>\n",
       "      <td>110.01</td>\n",
       "      <td>179.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.81</td>\n",
       "      <td>437.60</td>\n",
       "      <td>70.02</td>\n",
       "      <td>109.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>325.00</td>\n",
       "      <td>315.00</td>\n",
       "      <td>43.75</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>640.00</td>\n",
       "      <td>728.50</td>\n",
       "      <td>111.00</td>\n",
       "      <td>182.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>939.00</td>\n",
       "      <td>1089.25</td>\n",
       "      <td>169.00</td>\n",
       "      <td>272.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1830.00</td>\n",
       "      <td>297.00</td>\n",
       "      <td>457.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count      1880.00           1880.00           1880.00            1880.00\n",
       "mean        631.35            719.29            110.01             179.82\n",
       "std         348.81            437.60             70.02             109.40\n",
       "min           1.00             12.00              3.00               3.00\n",
       "25%         325.00            315.00             43.75              78.75\n",
       "50%         640.00            728.50            111.00             182.12\n",
       "75%         939.00           1089.25            169.00             272.31\n",
       "max        1208.00           1830.00            297.00             457.50"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 3,\n",
       "  'sentence_chunk': 'Human Nutrition: 2020 Edition UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM ALAN TITCHENAL, SKYLAR HARA, NOEMI ARCEO CAACBAY, WILLIAM MEINKE-LAU, YA-YUN YANG, MARIE KAINOA FIALKOWSKI REVILLA, JENNIFER DRAPER, GEMADY LANGFELDER, CHERYL GIBBY, CHYNA NICOLE CHUN, AND ALLISON CALABRESE',\n",
       "  'chunk_char_count': 308,\n",
       "  'chunk_word_count': 42,\n",
       "  'chunk_token_count': 77.0},\n",
       " {'page_number': 4,\n",
       "  'sentence_chunk': 'Human Nutrition: 2020 Edition by University of Hawai‘i at Mānoa Food Science and Human Nutrition Program is licensed under a Creative Commons Attribution 4.0 International License, except where otherwise noted.',\n",
       "  'chunk_char_count': 210,\n",
       "  'chunk_word_count': 30,\n",
       "  'chunk_token_count': 52.5},\n",
       " {'page_number': 5,\n",
       "  'sentence_chunk': 'Contents Preface xxv University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program About the Contributors xxvi University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program Acknowledgements xl University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program Part I. Chapter 1. Basic Concepts in Nutrition Introduction 3 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program Food Quality 14 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program Units of Measure 18 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program',\n",
       "  'chunk_char_count': 766,\n",
       "  'chunk_word_count': 116,\n",
       "  'chunk_token_count': 191.5},\n",
       " {'page_number': 6,\n",
       "  'sentence_chunk': 'Lifestyles and Nutrition 21 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program Achieving a Healthy Diet 30 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program Research and the Scientific Method 34 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program Types of Scientific Studies 41 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program Part II. Chapter 2. The Human Body Introduction 55 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program Basic Biology, Anatomy, and Physiology 62 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program The Digestive System 68 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program',\n",
       "  'chunk_char_count': 941,\n",
       "  'chunk_word_count': 144,\n",
       "  'chunk_token_count': 235.25},\n",
       " {'page_number': 7,\n",
       "  'sentence_chunk': 'The Cardiovascular System 82 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program Central Nervous System 94 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program The Respiratory System 99 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program The Endocrine System 106 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program The Urinary System 110 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program The Muscular System 117 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program The Skeletal System 120 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program The Immune System 129 University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program',\n",
       "  'chunk_char_count': 998,\n",
       "  'chunk_word_count': 152,\n",
       "  'chunk_token_count': 249.5}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunks that are under (30 tokens will be removed: experimental)\n",
    "# We will remove them as they may not have any need to be used => not usefull information may be provided by them: \n",
    "min_token_lenght = 30\n",
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_lenght].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_len[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding our Chunks:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
